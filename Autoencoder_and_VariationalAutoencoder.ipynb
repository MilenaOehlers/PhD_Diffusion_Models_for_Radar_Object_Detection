{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzay4ygpdiUYT/khfRj2zn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MilenaOehlers/generative_models_for_deep_radar_object_detection/blob/main/Autoencoder_and_VariationalAutoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following https://medium.com/@sofeikov/implementing-variational-autoencoders-from-scratch-533782d8eb95"
      ],
      "metadata": {
        "id": "HQ1VaV-Qg6Tw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "from urllib import request\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch"
      ],
      "metadata": {
        "id": "htcUTx_ThDmU"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGtSI_ChgoMK",
        "outputId": "147bb14f-79f5-4c3c-bb59-b617ba1a011d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading train-images-idx3-ubyte.gz\n",
            "Downloading train-labels-idx1-ubyte.gz\n",
            "Downloading t10k-images-idx3-ubyte.gz\n",
            "Downloading t10k-labels-idx1-ubyte.gz\n"
          ]
        }
      ],
      "source": [
        "# Download the files\n",
        "url = \"http://yann.lecun.com/exdb/mnist/\"\n",
        "filenames = ['train-images-idx3-ubyte.gz', 'train-labels-idx1-ubyte.gz',\n",
        "             't10k-images-idx3-ubyte.gz', 't10k-labels-idx1-ubyte.gz']\n",
        "data = []\n",
        "for filename in filenames:\n",
        "    print(\"Downloading\", filename)\n",
        "    request.urlretrieve(url + filename, filename)\n",
        "    with gzip.open(filename, 'rb') as f:\n",
        "        if 'labels' in filename:\n",
        "            # Load the labels as a one-dimensional array of integers\n",
        "            data.append(np.frombuffer(f.read(), np.uint8, offset=8))\n",
        "        else:\n",
        "            # Load the images as a two-dimensional array of pixels\n",
        "            data.append(np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1,28*28))\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, y_train, X_test, y_test = data\n",
        "\n",
        "# Normalize the pixel values\n",
        "X_train = X_train.astype(np.float32) / 255.0\n",
        "X_test = X_test.astype(np.float32) / 255.0\n",
        "\n",
        "# Convert labels to integers\n",
        "y_train = y_train.astype(np.int64)\n",
        "y_test = y_test.astype(np.int64)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show_images(images, labels):\n",
        "    \"\"\"\n",
        "    Display a set of images and their labels using matplotlib.\n",
        "    The first column of `images` should contain the image indices,\n",
        "    and the second column should contain the flattened image pixels\n",
        "    reshaped into 28x28 arrays.\n",
        "    \"\"\"\n",
        "    # Extract the image indices and reshaped pixels\n",
        "    pixels = images.reshape(-1, 28, 28)\n",
        "\n",
        "    # Create a figure with subplots for each image\n",
        "    fig, axs = plt.subplots(\n",
        "        ncols=len(images), nrows=1, figsize=(10, 3 * len(images))\n",
        "    )\n",
        "\n",
        "    # Loop over the images and display them with their labels\n",
        "    for i in range(len(images)):\n",
        "        # Display the image and its label\n",
        "        axs[i].imshow(pixels[i], cmap=\"gray\")\n",
        "        axs[i].set_title(\"Label: {}\".format(labels[i]))\n",
        "\n",
        "        # Remove the tick marks and axis labels\n",
        "        axs[i].set_xticks([])\n",
        "        axs[i].set_yticks([])\n",
        "        axs[i].set_xlabel(\"Index: {}\".format(i))\n",
        "\n",
        "    # Adjust the spacing between subplots\n",
        "    fig.subplots_adjust(hspace=0.5)\n",
        "\n",
        "    # Show the figure\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "jykMnw3eg0qB"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Set the number of hidden units\n",
        "        self.num_hidden = 8\n",
        "\n",
        "        # Define the encoder part of the autoencoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(784, 256),  # input size: 784, output size: 256\n",
        "            nn.ReLU(),  # apply the ReLU activation function\n",
        "            nn.Linear(256, self.num_hidden),  # input size: 256, output size: num_hidden\n",
        "            nn.ReLU(),  # apply the ReLU activation function\n",
        "        )\n",
        "\n",
        "        # Define the decoder part of the autoencoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(self.num_hidden, 256),  # input size: num_hidden, output size: 256\n",
        "            nn.ReLU(),  # apply the ReLU activation function\n",
        "            nn.Linear(256, 784),  # input size: 256, output size: 784\n",
        "            nn.Sigmoid(),  # apply the sigmoid activation function to compress the output to a range of (0, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass the input through the encoder\n",
        "        encoded = self.encoder(x)\n",
        "        # Pass the encoded representation through the decoder\n",
        "        decoded = self.decoder(encoded)\n",
        "        # Return both the encoded representation and the reconstructed output\n",
        "        return encoded, decoded"
      ],
      "metadata": {
        "id": "YnNYtt7chzow"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WiqhsC1MkiaQ"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ae(X_train, learning_rate=0.001,num_epochs=10,device=\"cpu\",\n",
        "             batch_size=32):\n",
        "  # Convert the training data to PyTorch tensors\n",
        "  X_train = torch.from_numpy(X_train)\n",
        "\n",
        "  # Create the autoencoder model and optimizer\n",
        "  model = AutoEncoder()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  # Define the loss function\n",
        "  criterion = nn.MSELoss()\n",
        "\n",
        "  # Set the device to GPU if available, otherwise use CPU\n",
        "  model.to(device)\n",
        "\n",
        "  # Create a DataLoader to handle batching of the training data\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "      X_train, batch_size=batch_size, shuffle=True\n",
        "  )\n",
        "  # Training loop\n",
        "  for epoch in range(num_epochs):\n",
        "      total_loss = 0.0\n",
        "      for batch_idx, data in enumerate(train_loader):\n",
        "          # Get a batch of training data and move it to the device\n",
        "          data = data.to(device)\n",
        "\n",
        "          # Forward pass\n",
        "          encoded, decoded = model(data)\n",
        "\n",
        "          # Compute the loss and perform backpropagation\n",
        "          loss = criterion(decoded, data)\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          # Update the running loss\n",
        "          total_loss += loss.item() * data.size(0)\n",
        "\n",
        "      # Print the epoch loss\n",
        "      epoch_loss = total_loss / len(train_loader.dataset)\n",
        "      print(\n",
        "          \"Epoch {}/{}: loss={:.4f}\".format(epoch + 1, num_epochs, epoch_loss)\n",
        "      )\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "fOUbJdC2hzl1"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(AutoEncoder):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Add mu and log_var layers for reparameterization\n",
        "        self.mu = nn.Linear(self.num_hidden, self.num_hidden)\n",
        "        self.log_var = nn.Linear(self.num_hidden, self.num_hidden)\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        # Compute the standard deviation from the log variance\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        # Generate random noise using the same shape as std\n",
        "        eps = torch.randn_like(std)\n",
        "        # Return the reparameterized sample\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass the input through the encoder\n",
        "        encoded = self.encoder(x)\n",
        "        # Compute the mean and log variance vectors\n",
        "        mu = self.mu(encoded)\n",
        "        log_var = self.log_var(encoded)\n",
        "        # Reparameterize the latent variable\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        # Pass the latent variable through the decoder\n",
        "        decoded = self.decoder(z)\n",
        "        # Return the encoded output, decoded output, mean, and log variance\n",
        "        return encoded, decoded, mu, log_var\n",
        "\n",
        "    def sample(self, num_samples):\n",
        "        with torch.no_grad():\n",
        "            # Generate random noise\n",
        "            z = torch.randn(num_samples, self.num_hidden).to(device)\n",
        "            # Pass the noise through the decoder to generate samples\n",
        "            samples = self.decoder(z)\n",
        "        # Return the generated samples\n",
        "        return samples"
      ],
      "metadata": {
        "id": "WO5RPfz1hzjN"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a loss function that combines binary cross-entropy and Kullback-Leibler divergence\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    # Compute the binary cross-entropy loss between the reconstructed output and the input data\n",
        "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction=\"sum\")\n",
        "    # Compute the Kullback-Leibler divergence between the learned latent variable distribution and a standard Gaussian distribution\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    # Combine the two losses by adding them together and return the result\n",
        "    return BCE + KLD"
      ],
      "metadata": {
        "id": "IzDpGb0dhzg8"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_vae(X_train, learning_rate=1e-3, num_epochs=10, batch_size=32):\n",
        "    # Convert the training data to PyTorch tensors\n",
        "    X_train = torch.from_numpy(X_train).to(device)\n",
        "\n",
        "    # Create the autoencoder model and optimizer\n",
        "    model = VAE()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Define the loss function\n",
        "    criterion = nn.MSELoss(reduction=\"sum\")\n",
        "\n",
        "    # Set the device to GPU if available, otherwise use CPU\n",
        "    model.to(device)\n",
        "\n",
        "    # Create a DataLoader to handle batching of the training data\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        X_train, batch_size=batch_size, shuffle=True\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        for batch_idx, data in enumerate(train_loader):\n",
        "            # Get a batch of training data and move it to the device\n",
        "            data = data.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            encoded, decoded, mu, log_var = model(data)\n",
        "\n",
        "            # Compute the loss and perform backpropagation\n",
        "            KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "            loss = criterion(decoded, data) + 3 * KLD\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update the running loss\n",
        "            total_loss += loss.item() * data.size(0)\n",
        "\n",
        "        # Print the epoch loss\n",
        "        epoch_loss = total_loss / len(train_loader.dataset)\n",
        "        print(\n",
        "            \"Epoch {}/{}: loss={:.4f}\".format(epoch + 1, num_epochs, epoch_loss)\n",
        "        )\n",
        "\n",
        "    # Return the trained model\n",
        "    return model"
      ],
      "metadata": {
        "id": "feNft5kGhzej"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ae_model = train_ae(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VM1PsJKQtnVn",
        "outputId": "1f416582-d97e-4a35-b793-406b73526f52"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10: loss=0.0390\n",
            "Epoch 2/10: loss=0.0308\n",
            "Epoch 3/10: loss=0.0269\n",
            "Epoch 4/10: loss=0.0254\n",
            "Epoch 5/10: loss=0.0246\n",
            "Epoch 6/10: loss=0.0241\n",
            "Epoch 7/10: loss=0.0237\n",
            "Epoch 8/10: loss=0.0233\n",
            "Epoch 9/10: loss=0.0230\n",
            "Epoch 10/10: loss=0.0228\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vae_model = train_vae(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdDqxSnPhzcK",
        "outputId": "6f7119c2-55ca-4d8e-dddb-2f1d15e8a862"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10: loss=1623.8221\n",
            "Epoch 2/10: loss=1484.7464\n",
            "Epoch 3/10: loss=1462.6265\n",
            "Epoch 4/10: loss=1447.3864\n",
            "Epoch 5/10: loss=1436.3919\n",
            "Epoch 6/10: loss=1428.9828\n",
            "Epoch 7/10: loss=1423.6197\n",
            "Epoch 8/10: loss=1418.4951\n",
            "Epoch 9/10: loss=1415.6623\n",
            "Epoch 10/10: loss=1412.5767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oP6aHnH0hzTT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}